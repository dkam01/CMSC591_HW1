---
title: Homework 1 - Open Source Tools
author:
    - name: Dhilan Kamani
      email: kamanids@vcu.edu
format:
    html:
        embed-resources: true
        html-math-method: katex
        theme: spacelab
        toc: true

## Useful references:

# - Basic Markdown: https://quarto.org/docs/authoring/markdown-basics.html
# - Quarto figures: https://quarto.org/docs/authoring/figures.html
# - HTML document basics: https://quarto.org/docs/output-formats/html-basics.html
# - Quarto guide: https://quarto.org/docs/guide/
# - VS Code and Quarto: https://quarto.org/docs/tools/vscode.html
#   (RTFM and GET THE EXTENSION!!!!)

---

This report provides an exploration into the new landscape of data engineering tools as of 2024. It overviews the nine major categories, as Mr. Sadeghi outlines in his article, as well as a more detailed exploration of some of the tool subcategories, specifically looking at subcategories important to applications in bioinformatics.

# Open Source Data Engineering Tools

Author Alireza Sadeghi offers a nice overview of the [2024 data engineering landscape](https://practicaldataengineering.substack.com/p/open-source-data-engineering-landscape) in the on-line web site [Practical Data Engineering Substack](https://practicaldataengineering.substack.com/).

![](assets/tools-2024.webp)

# Major Categories

Mr. Sadeghi proposals nine major tools categories.

## Storage Systems

Storage systems are foundational components in data engineering that handle the storage and retrieval of data. These systems can include traditional databases, distributed storage solutions, and modern cloud-based storage options. They provide scalable, reliable, and secure environments to store structured, semi-structured, and unstructured data, ensuring data is accessible for processing, analysis, and other operations.

## Data Lake Platform

Data Lake platforms are designed to store vast amounts of raw data in its native format until it is needed. Unlike traditional databases, data lakes can store structured, semi-structured, and unstructured data, providing a flexible and scalable storage solution. These platforms support the integration of multiple data sources, making it easier to perform big data analytics and machine learning on large datasets.

## Data Integration

Data Integration tools enable the combination of data from different sources and formats in a united view. These tools are used for ETL processes (Extract, Transform, Load) as well as migration and synchronization of data. Multiple data sources are supported, such as, databases, APIs, file systems, and cloud services.

## Data Processing and Computation

Data processing and computation tools are used to transform, analyze, and compute large volumes of data. These tools support both batch and real-time processing, enabling organizations to perform complex calculations, data transformations, and aggregations. 

## Workflow and DataOps

Workflow and Data Ops tools help automate, orchestrate, and manage data pipelines and workflows. These tools provide scheduling, monitoring, and error-handling capabilities to ensure that data processes run smoothly and efficiently. They are essential for maintaining data pipeline reliability, scalability, and resilience in production environments.

## Data Infrastructure and Monitoring

Data infrastructure and monitoring tools provide the foundational components and oversight necessary to maintain and optimize data systems. These tools offer infrastructure management, resource allocation, performance monitoring, and alerting capabilities. They are crucial for ensuring data systems are running efficiently and identifying issues before they impact business operations.

## ML/AI Platform

ML/AI platforms provide the infrastructure and tools required to build, train, deploy, and manage machine learning and artificial intelligence models. These platforms facilitate data preparation, model experimentation, and operationalization, supporting end-to-end ML workflows. They often include features like model monitoring, versioning, and governance to ensure effective ML model management.

## Metadata Management

Metadata management tools help in organizing, managing, and utilizing metadata, which is data about data. These tools provide capabilities for data cataloging, lineage tracking, and metadata governance, making it easier for organizations to understand their data assets. Effective metadata management enhances data discoverability, quality, and compliance.

## Analytics and Visualization

Analytics and visualization tools enable the exploration and analysis of data through interactive dashboards, reports, and visualizations. These tools allow users to derive insights from data, create data-driven stories, and make informed decisions. They support various data sources and offer features like real-time analytics, predictive modeling, and data exploration.

# Subcategory Exploration

In the following sections I identify three subcategories of data engineering tools of greatest interest to me, specifically how they can be used in bioinformatics.

## Distributed File Systems (DFS)

### Basic Description

A Distributed File System (DFS) is a file storage system that manages files across multiple servers, making it appear as a single file system to the user. DFSs are designed to handle large volumes of data by distributing the storage load across many machines, allowing for scalable, fault-tolerant, and highly available storage solutions. They enable efficient storage and retrieval of massive datasets, typically in a data lake architecture, by supporting large-scale data processing and analytics.

### Importance

DFSs are crucial for modern data lake platforms because they allow organizations to store and manage vast amounts of data that can be structured, semi-structured, or unstructured. They provide high availability and reliability by replicating data across multiple servers, ensuring data is protected against hardware failures. Additionally, DFSs enable parallel processing, which is essential for big data analytics, machine learning, and large-scale data processing tasks.

### Comparison to Relational OLTP databases

DFSs are fundamentally different from Relational Online Transaction Processing (OLTP) databases in several ways. While OLTP databases are designed for handling transactional data with ACID (Atomicity, Consistency, Isolation, Durability) properties, DFSs are optimized for storing large-scale, unstructured or semi-structured data with eventual consistency. OLTP databases typically run on a single server or a small cluster, focusing on quick, real-time updates and queries, whereas DFSs are designed to distribute data across many servers, optimizing for throughput and fault tolerance rather than low-latency transactions.

### Bioinformatics Application

In bioinformatics, DFSs are invaluable for storing and managing large datasets such as genomic sequences, protein structures, and various biological data types. For example, genomic data generated from next-generation sequencing (NGS) can be several terabytes or even petabytes in size, necessitating a distributed storage solution. DFSs allow bioinformaticians to store vast amounts of raw and processed data in a single, accessible location, supporting collaborative research and enabling efficient data sharing, parallel processing, and large-scale data analysis workflows.

## Parallel Python Processing

### Basic Description

Parallel Python Execution refers to the ability to run Python code concurrently across multiple processors or cores, enabling faster execution of computationally intensive tasks. This approach leverages parallel computing techniques to distribute workloads across several processing units, significantly reducing the time required to perform large-scale data processing or complex computations. Parallel Python Execution is often facilitated by libraries such as multiprocessing, joblib, Dask, and PySpark.

### Importance

Parallel Python Execution is essential for data processing and computation because it maximizes the use of available computational resources, leading to more efficient and quicker processing times. By parallelizing tasks, it allows data engineers and scientists to handle larger datasets and perform more complex analyses in a fraction of the time required by single-threaded execution. This capability is particularly valuable in fields that require heavy computational loads, such as machine learning, scientific computing, and big data analytics.

### Comparison to Relational OLTP databases

Relational OLTP databases are designed for managing transactions and supporting CRUD (Create, Read, Update, Delete) operations with a focus on maintaining data integrity and consistency through ACID transactions. In contrast, Parallel Python Execution is not concerned with transaction management or data consistency in a database context. Instead, it focuses on accelerating computation by breaking tasks into smaller chunks that can be executed simultaneously. While OLTP databases aim for quick, real-time response times for individual queries, Parallel Python Execution is about maximizing throughput for data processing tasks by leveraging concurrency.

### Bioinformatics Application

In bioinformatics, Parallel Python Execution can be applied to accelerate various computational tasks, such as sequence alignment, genome assembly, and phylogenetic analysis, which are computationally intensive and often involve large datasets. For instance, parallelizing the execution of algorithms for comparing DNA sequences across multiple cores can significantly reduce the processing time required for large-scale genomic studies. This capability allows researchers to run more experiments in less time, increasing the efficiency of data analysis and accelerating scientific discoveries.

## Metadata Platforms

### Basic Description

A Metadata Platform is a system that provides tools and functionalities for managing metadata, which is data about other data. These platforms offer capabilities for cataloging, discovering, and governing data assets, helping organizations maintain a comprehensive view of their data landscape. They typically include features for tracking data lineage, auditing data usage, and managing data access policies, ensuring that metadata is accurate, up-to-date, and accessible to users.

### Importance

Metadata Platforms are vital because they enable organizations to understand their data assets, improve data quality, and ensure compliance with data governance policies. By providing a centralized repository for metadata, these platforms make it easier for data engineers, analysts, and scientists to discover and understand the data available to them, promoting better data utilization and reducing the time spent searching for or cleaning data. Metadata management is also crucial for ensuring transparency and accountability in data handling, which is increasingly important for regulatory compliance.

### Comparison to Relational OLTP databases

While Relational OLTP databases focus on storing and managing data with a structure defined by tables and schemas, a Metadata Platform is concerned with managing the data about that data, such as its origin, structure, usage, and transformation history. OLTP databases typically support fast, transaction-based access to data but do not provide comprehensive tools for tracking data lineage, cataloging datasets, or managing data governance policies. In contrast, Metadata Platforms are designed to provide these capabilities, enabling a more holistic view of data assets across an organization.

### Bioinformatics Application

In bioinformatics, Metadata Platforms can be applied to manage the vast amount of metadata associated with biological data, such as sequencing data, experimental results, and computational models. These platforms help researchers and scientists track the provenance of datasets, understand the transformations applied to raw data, and maintain compliance with data-sharing regulations. For example, a Metadata Platform can catalog genomic datasets along with their associated metadata, such as sample source, sequencing method, and quality metrics, facilitating more effective data management and enabling reproducible research in bioinformatics.

# Reflection

I like the introductory nature of this project. It requires a base level of understanding, but the nature of an exploration through all of these types of data engineering tools is useful, because prior to this project, I'd say my knowledge on the topic was minimal. While I'm not an expert now, having to see which categories of tools would be of more or less use to me specifically was extremely informative, given that I am used to working with different types of data and files than your typical CMSC student.

The hardest thing for me to accomplish was selecting the subcategories to delve into deeper. I wanted to choose categories with tools that are more useful to bioinformatics, specifically the handling and analysis of data.

There weren't any surprises on this project. I don't think I realized how many tools there were that essentially accomplish the same tasks, with only one or two functionalities that make one tool better than another for a specific task, but it wasn't a surprise.

I wouldn't, I think I handled the project well, seeing as it was pretty much just research. One thing would be to actually check my storage capacity and be better about cleaning junk files from my computer.